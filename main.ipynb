{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc1d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.optim.swa_utils import AveragedModel, update_bn\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "seed_everything(7)\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 256 if AVAIL_GPUS else 64\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger=WandbLogger(project=\"full_attention\", name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10886f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.RandomCrop(32, padding=4),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        cifar10_normalization(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        cifar10_normalization(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cifar10_dm = CIFAR10DataModule(\n",
    "    data_dir=PATH_DATASETS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    train_transforms=train_transforms,\n",
    "    test_transforms=test_transforms,\n",
    "    val_transforms=test_transforms,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c55ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "\n",
    "class LitResnet(LightningModule):\n",
    "    def __init__(self, lr=0.05):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.fe = torch.nn.Sequential(*(list(create_model().children())[:-1]))\n",
    "        self.fc = nn.Sequential(nn.Linear(512, 10))\n",
    "        \n",
    "        C = 512 # feature dim\n",
    "        K = 1280 # number of keys in dict\n",
    "        self.queue = torch.randn((C, K))# .to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fe(x)\n",
    "        out = torch.flatten(out, 1) # 256x512 (batchxfeats; feats=C)\n",
    "        # out_w_queue = self.cat_queue_to_out(out)\n",
    "        self.update_queue(out.detach())\n",
    "        out = self.fc(out)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc)\n",
    "        \n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "            self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        steps_per_epoch = 45000 // BATCH_SIZE\n",
    "        scheduler_dict = {\n",
    "            \"scheduler\": OneCycleLR(\n",
    "                optimizer,\n",
    "                0.1,\n",
    "                epochs=self.trainer.max_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}\n",
    "    \n",
    "    ### ðŸ§ª full attention stuff ###\n",
    "    def cat_queue_to_out(self, out):\n",
    "        queue_block = self.queue.clone().unsqueeze(0).repeat(256, 1, 1) # make queue torch.Size([256, 512, 1280])\n",
    "        out_w_queue = torch.cat((out.cpu().unsqueeze(-1), queue_block), -1) # 256x512x1281\n",
    "        return out_w_queue\n",
    "    \n",
    "    def update_queue(self, k):\n",
    "        # queue is CxK\n",
    "        # need to roll by batch size, then replace with curr batch\n",
    "        \n",
    "        # k.t() is CxN\n",
    "        batch_size = k.shape[0] # N\n",
    "        \n",
    "        self.queue = self.queue.roll(batch_size, 1)\n",
    "        self.queue[:, :batch_size] = k.t()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c15ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = LitResnet(lr=0.05)\n",
    "model.datamodule = cifar10_dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa26892",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.queue.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5fa3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    logger=logger,\n",
    "    max_epochs=30,\n",
    "    gpus=AVAIL_GPUS,\n",
    "    callbacks=[LearningRateMonitor(logging_interval=\"step\")],\n",
    ")\n",
    "\n",
    "trainer.fit(model, cifar10_dm)\n",
    "trainer.test(model, datamodule=cifar10_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac81edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
