{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc1d0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.optim.swa_utils import AveragedModel, update_bn\n",
    "from torchmetrics.functional import accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "seed_everything(7)\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 256 if AVAIL_GPUS else 64\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de290cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44c0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger=None\n",
    "logger=WandbLogger(project=\"full_attention\", name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d10886f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shats/miniconda3/envs/moti/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:73: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "/home/shats/miniconda3/envs/moti/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:77: LightningDeprecationWarning: DataModule property `val_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "/home/shats/miniconda3/envs/moti/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:81: LightningDeprecationWarning: DataModule property `test_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "train_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.RandomCrop(32, padding=4),\n",
    "        torchvision.transforms.RandomHorizontalFlip(),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        cifar10_normalization(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        cifar10_normalization(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cifar10_dm = CIFAR10DataModule(\n",
    "    data_dir=PATH_DATASETS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    train_transforms=train_transforms,\n",
    "    test_transforms=test_transforms,\n",
    "    val_transforms=test_transforms,\n",
    "    drop_last=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c55ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "\n",
    "class LitResnet(LightningModule):\n",
    "    def __init__(self, lr=0.05):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.fe = torch.nn.Sequential(*(list(create_model().children())[:-1]))\n",
    "        self.cnn = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=2),\n",
    "            # torch.nn.BatchNorm1d(num_channels*2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=2),\n",
    "            # torch.nn.BatchNorm1d(num_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=8, out_channels=1, kernel_size=3, stride=2),\n",
    "            # torch.nn.BatchNorm1d(num_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=3),\n",
    "        )\n",
    "        self.fc = nn.Sequential(nn.Linear(1060, 10))\n",
    "        C = 512 # feature dim\n",
    "        K = 1280 # number of keys in dict\n",
    "        # self.queue = torch.randn((C, K))# .to(device)\n",
    "        self.queue = torch.zeros((C, K))# .to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fe(x)\n",
    "        out = torch.flatten(out, 1) # 256x512 (batchxfeats; feats=C)\n",
    "        # out_w_queue = self.cat_queue_to_out(out)\n",
    "        self.update_queue(out.detach())\n",
    "        out = self.cat_queue_to_out(out).unsqueeze(1).cuda() # unsqueeze for C dim\n",
    "        out = self.cnn(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "            self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        steps_per_epoch = 45000 // BATCH_SIZE\n",
    "        scheduler_dict = {\n",
    "            \"scheduler\": OneCycleLR(\n",
    "                optimizer,\n",
    "                0.1,\n",
    "                epochs=self.trainer.max_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}\n",
    "    \n",
    "    ### ðŸ§ª full attention stuff ###\n",
    "    def cat_queue_to_out(self, out):\n",
    "        queue_block = self.queue.clone().unsqueeze(0).repeat(256, 1, 1) # make queue torch.Size([256, 512, 1280])\n",
    "        out_w_queue = torch.cat((out.cpu().unsqueeze(-1), queue_block), -1) # 256x512x1281\n",
    "        return out_w_queue\n",
    "    \n",
    "    def update_queue(self, k):\n",
    "        # queue is CxK\n",
    "        # need to roll by batch size, then replace with curr batch\n",
    "        \n",
    "        # k.t() is CxN\n",
    "        batch_size = k.shape[0] # N\n",
    "        \n",
    "        self.queue = self.queue.roll(batch_size, 1)\n",
    "        self.queue[:, :batch_size] = k.t()\n",
    "        \n",
    "        # plt.imshow(self.queue, cmap='hot', interpolation='nearest')\n",
    "        # plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51a6620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FROM lucidrains github ###\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e7c15ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = LitResnet(lr=0.05)\n",
    "model.datamodule = cifar10_dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fa26892",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1280])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.queue.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd5fa3b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shats/miniconda3/envs/moti/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:114: LightningDeprecationWarning: DataModule property `train_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "/home/shats/miniconda3/envs/moti/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:133: LightningDeprecationWarning: DataModule property `val_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshatz\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.18 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/shatz/full_attention/runs/b18gn506\" target=\"_blank\">dainty-smoke-15</a></strong> to <a href=\"https://wandb.ai/shatz/full_attention\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type       | Params\n",
      "------------------------------------\n",
      "0 | fe   | Sequential | 11.2 M\n",
      "1 | cnn  | Sequential | 1.6 K \n",
      "2 | fc   | Sequential | 10.6 K\n",
      "------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.724    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b1abab37ae40f49ad8a1ef7132ab3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shats/miniconda3/envs/moti/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "/home/shats/miniconda3/envs/moti/lib/python3.9/site-packages/pytorch_lightning/core/datamodule.py:152: LightningDeprecationWarning: DataModule property `test_transforms` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f168ff82f9142e6bcdbc5906571737c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.09985977411270142, 'test_loss': 2.3029186725616455}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/queues.py\", line 251, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 205, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 416, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/shats/miniconda3/envs/moti/lib/python3.9/multiprocessing/connection.py\", line 373, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 2.3029186725616455, 'test_acc': 0.09985977411270142}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    logger=logger,\n",
    "    max_epochs=30,\n",
    "    gpus=AVAIL_GPUS,\n",
    "    callbacks=[LearningRateMonitor(logging_interval=\"step\")],\n",
    ")\n",
    "\n",
    "trainer.fit(model, cifar10_dm)\n",
    "trainer.test(model, datamodule=cifar10_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac81edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f130e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2f9896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04685bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.random.random((16, 16))\n",
    "plt.imshow(a, cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c893b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
